{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL20-fall-seminar6.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dv2skd3-53rx"
      },
      "source": [
        "# Нейросети в задачих обработки текстов\n",
        "\n",
        "**Разработчик: Алексей Озерин, Ирина Сапарина**",
	"\n",
	"[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/aosokin/dl_cshse_ami/blob/master/2021-fall/homeworks_small/shw6/DL21-fall-shw6.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozLuJF3kIaPF"
      },
      "source": [
        "# Генерация коротких текстов с помощью Transformer\n",
        "\n",
        "\n",
        "Генерировать тексты можно как с помощью RNN, так и с помощью Transformer, предсказывая следующий символ последовательности по предыдущим. Мы будем использовать архитектуру Transformer.\n",
        "\n",
        "В этом задании предлагается написать и проучить на небольшом датасете имен генеративную модель на основе символов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhrsBW44Q70a"
      },
      "outputs": [],
      "source": [
         "# Load dependencies\n",
         "!wget --quiet --show-progress \"https://raw.githubusercontent.com/aosokin/dl_cshse_ami/master/2021-fall/homeworks_small/shw6/names\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_s_Z5lbIaPG"
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6nXxU8WIaPM"
      },
      "source": [
        "В файле `names` находится ~8k имен на латинице.\n",
        "\n",
        "Модель будет получать на вход имя `Amandy` и выдавать его же, только со сдвигом: `mandy `.\n",
        "\n",
        "Чтобы сеть училась генерировать заглавные буквы, добавим в начало специальный токен `_`.\n",
        "\n",
        "Также нам потребуется правило для останова генерации (это может быть просто ограничение на количество шагов). С другой стороны, можно добавить в конец каждого примера обучающей выборки специальный `<EOS>` токен. В данном случае обозначим его `#`:\n",
        "\n",
        "```\n",
        "_Amandy --> Amandy#\n",
        "```\n",
        "\n",
        "Можно прекращать генерацию при досрочном выпадании `<EOS>`.\n",
        "\n",
        "Для генерации на каждом шаге будем подавать на вход букву, предсказанную на предыдущем.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFRHva2zIaPN"
      },
      "source": [
        "import os\n",
        "start_token = \"_\"\n",
        "eos = '#'\n",
        "\n",
        "with open(\"names\") as f:\n",
        "    names = f.readlines()\n",
        "    names = [start_token + name.strip() + eos for name in names]\n",
        "\n",
        "names = list(set(names))  # в датасете есть повторы\n",
        "print('There are {} names: '.format(len(names)))\n",
        "for x in names[::1000]:\n",
        "    print(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RizB5cBTIaPP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSve0HBaIaPS"
      },
      "source": [
        "# TODO: постройте частоты употреблений букв\n",
        "<your code>\n",
        "# HINT: для графика возьмите plt.bar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAeSKss4IaPV"
      },
      "source": [
        "# в датасете есть слова с разными длинами\n",
        "MAX_LENGTH = max(map(len,names))\n",
        "print(\"max length =\", MAX_LENGTH)\n",
        "\n",
        "plt.title('Sequence length distribution')\n",
        "plt.hist(list(map(len,names)), bins=25);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWnDPWr9IaPY"
      },
      "source": [
        "names[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgB0VE9BIaPa"
      },
      "source": [
        "# TODO: отберите уникальные токены и заполните два словаря для конвертации токенов <-> индексы\n",
        "# сделайте так, чтобы start_token имел номер 0\n",
        "    \n",
        "tokens = <your code>\n",
        "    \n",
        "tok2id = <your code>\n",
        "id2tok = <your code>\n",
        "\n",
        "n_tokens = len(tokens)\n",
        "print ('There are {} tokens',n_tokens)\n",
        "\n",
        "assert 50 < n_tokens < 60\n",
        "\n",
        "print('Vocabular: ' + \"\".join(tokens))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jF_ukJotIaPd"
      },
      "source": [
        "def to_matrix(names, max_len=None, pad=tok2id[' '], dtype=np.int64):\n",
        "    \"\"\"Casts a list of names into matrix\"\"\"\n",
        "    \n",
        "    max_len = max_len or max(map(len, names))\n",
        "    names_ix = np.zeros([len(names), max_len], dtype) + pad\n",
        "\n",
        "    for i in range(len(names)):\n",
        "        name_ix = list(map(tok2id.get, names[i]))\n",
        "        names_ix[i, :len(name_ix)] = name_ix\n",
        "\n",
        "    return names_ix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmg_il5MIaPg"
      },
      "source": [
        "print('\\n'.join(names[:10]))\n",
        "print(to_matrix(names[:10]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64C8xOqCIaPk"
      },
      "source": [
        "# TODO: разбейте все имена на тренировочную (80%) и тестовую часть (20%)\n",
        "<your code>\n",
        "\n",
        "train_data, val_data = split_data(names)\n",
        "\n",
        "len(train_data), len(val_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fW62jy6xIaPm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfGnm2QoIaPo"
      },
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch import optim\n",
        "from IPython.display import clear_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhgqoEHOIaPr"
      },
      "source": [
        "# Char-Transformer для имен\n",
        "\n",
        "Вам нужно написать сеть, кодирующую входные символы и их позиции с помощью таблиц Embeddings. \n",
        "Получившиеся тензоры пропустить через `TransformerEncoder`, затем преобразовать в логиты для предсказания новых символов.\n",
        "\n",
        "Transformer может обрабатывать сразу всю последовательность за один проход. Для того, чтобы у модели не было возможности \"заглянуть в будущее\", то есть использовать информацию о впреди идущих символах, необходимо сгенерировать маску. `TransformerEncoder` должен принимать на вход последовательность символов и маску.    \n",
        "![Transformer](https://drive.google.com/uc?export=view&id=1gXILzT3mGgc0mGlvqY-6R4bGs3Lx2YxM)\n",
        "Картинка из [illustrated transformer](http://jalammar.github.io/illustrated-transformer/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJCf0LYIIaPt"
      },
      "source": [
        "# TODO: заполните пропуски\n",
        "\n",
        "class NameTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_size, hidden_size, n_layers=2, n_head=2, dropout=0.1):\n",
        "        super(NameTransformer, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        <your code>\n",
        "        \n",
        "        self.register_buffer(\"position_ids\", torch.arange(MAX_LENGTH).unsqueeze(1))\n",
        "\n",
        "    def _generate_square_subsequent_mask(self, seq_len):\n",
        "        # TODO: сгенерируйте маску размера seq_len x seq_len\n",
        "        # если во время кодирования i-го символа j-й символ доступен, \n",
        "        # то (i,j) элемент маски равен 0, иначе -inf\n",
        "        \n",
        "        <your code>\n",
        "\n",
        "        return mask\n",
        "        \n",
        "    def forward(self, input):\n",
        "\n",
        "        <your code>\n",
        "\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmkgMHc8IaPu"
      },
      "source": [
        "# Код для тренировки"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S164svO9IaPw"
      },
      "source": [
        "def train_epoch(model, optimizer, train_batches):\n",
        "    loss_log = []\n",
        "    model.train()\n",
        "    \n",
        "    for batch in train_batches:\n",
        "        \n",
        "        nums = to_matrix(batch)\n",
        "        <your code>\n",
        "            \n",
        "        loss = loss.item()\n",
        "        loss_log.append(loss)\n",
        "    return loss_log   \n",
        "\n",
        "def test(model, test_batches):\n",
        "    loss_log = []\n",
        "    model.eval()\n",
        "    for batch in test_batches:  \n",
        "        \n",
        "        nums = to_matrix(batch)\n",
        "        <your code>\n",
        "        \n",
        "        loss = loss.item()\n",
        "        loss_log.append(loss)\n",
        "    return loss_log\n",
        "\n",
        "def plot_history(train_history, val_history, title='loss'):\n",
        "    plt.figure()\n",
        "    plt.title('{}'.format(title))\n",
        "    plt.plot(train_history, label='train', zorder=1)    \n",
        "    points = np.array(val_history)\n",
        "    plt.scatter(points[:, 0], points[:, 1], marker='+', s=180, c='orange', label='val', zorder=2)\n",
        "    plt.xlabel('train steps')\n",
        "    plt.legend(loc='best')\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "    \n",
        "def train(model, opt, n_epochs):\n",
        "    train_log = []\n",
        "    val_log = []\n",
        "    \n",
        "    bs = 32\n",
        "    total_steps = 0\n",
        "    train_batches = np.array_split(train_data, len(train_data) // bs)\n",
        "    test_batches = np.array_split(val_data, len(val_data) // bs)\n",
        "    for epoch in range(n_epochs):\n",
        "        train_loss = train_epoch(model, opt, train_batches)\n",
        "        train_log.extend(train_loss)\n",
        "        total_steps += len(train_batches)\n",
        "        \n",
        "        val_loss = test(model, test_batches)\n",
        "        train_log.extend(train_loss)\n",
        "        \n",
        "        val_log.append((len(train_log), np.mean(val_loss)))\n",
        "        \n",
        "        clear_output()\n",
        "        plot_history(train_log, val_log)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sxrc0a10IaPy"
      },
      "source": [
        "model = NameTransformer(len(tokens), 64, 64, n_layers=2, n_head=2, dropout=0.1)\n",
        "\n",
        "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "train(model, opt, 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffTAktWAIaP5"
      },
      "source": [
        "# Генерация по argmax"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugRlkX2ZIaP6"
      },
      "source": [
        "# Напишите функцию генерации продолжения строки\n",
        "def pick_by_argmax(logits):\n",
        "    <your code>\n",
        "\n",
        "def ids2string(ids):\n",
        "    return \"\".join(id2tok[_] for _ in ids)\n",
        "\n",
        "\n",
        "def gen_continuation(model, prefix=\"_\"):\n",
        "    nums = to_matrix(prefix)\n",
        "    nums = torch.from_numpy(nums)\n",
        "    \n",
        "    # TODO: сначала подайте на вход префикс\n",
        "    # нас интересует последний output, чтобы получить первое предсказание\n",
        "    <your code>\n",
        "    \n",
        "    # TODO: затем сгенерируйте несколько последующих символов\n",
        "    # outs -- это массив с номерами токенов\n",
        "    <your code>\n",
        "    \n",
        "    print(prefix + '|'+ ids2string(outs))\n",
        "    \n",
        "gen_continuation(model, \" Ku\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00547AA-IaP_"
      },
      "source": [
        "# Генерация с семплированием\n",
        "\n",
        "Обычный софтмакс \n",
        "$$p_i = \\frac{\\exp (x_i)}{\\sum \\exp (x_j)}$$\n",
        "можно модернизировать с помощью температуры:\n",
        "$$p_i = \\frac{\\exp (x_i / T)}{\\sum \\exp (x_j / T)}$$\n",
        "\n",
        "Это позволит плавно переходить от выбора наиболее вероятного элемента ($T << 1$) до практически равновероятного ($T >> 1$)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71cOcFxpIaQA"
      },
      "source": [
        "# Напишите функцию генерации батчами с семплированием из распределения и температурой\n",
        "def batch2string(ids, prefix):\n",
        "    # модифицируйте ids2string для работы с батчами\n",
        "    <your code>\n",
        "\n",
        "def pick_by_distribution(logits):\n",
        "    # превратите логиты в распределение\n",
        "    # затем семлируйте из него batch примеров\n",
        "    <your code>\n",
        "\n",
        "\n",
        "def gen_continuation_temp(model, prefix=\"_\", temperature=1.0, n=10):\n",
        "    nums = to_matrix([prefix] * n)\n",
        "    nums = torch.from_numpy(nums)\n",
        "\n",
        "    # аналогично, сначала подайте на вход префикс\n",
        "    # нас интересует последний output, чтобы получить первое предсказание\n",
        "    <your code>\n",
        "    \n",
        "    # затем, сгенерируйте n последующих символов\n",
        "    # в outs положите матрицу номеров токенов и отобразите ее\n",
        "    \n",
        "    print(batch2string(outs, prefix + '|'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pElLbEKIaQD"
      },
      "source": [
        "gen_continuation_temp(model, prefix=\" An\", temperature=0.5, n=10)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
