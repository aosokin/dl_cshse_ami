{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"},"colab":{"name":"DL21-fall-shw5.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"ysOZpjoNuk-K"},"source":["# Сверточные сети в задачах компьютерного зрения\n","\n","**Разработчики: Артем Бабенко, Екатерина Глазкова**"]},{"cell_type":"markdown","metadata":{"id":"nhrsBW44Q70a"},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/aosokin/dl_cshse_ami/blob/master/2021-fall/homeworks_small/shw5/DL21-fall-shw5.ipynb)"]},{"cell_type":"markdown","metadata":{"id":"8PrGzEGmuk-L"},"source":["На этом семинаре необходимо будет (1) реализовать простейшую metric learning архитектуру на основе сиамской нейросети с Contrastive Loss и (2) обучить модель UNet для задачи сегментации изображения. "]},{"cell_type":"code","metadata":{"id":"4X32hjkDRLDl"},"source":["!wget --quiet --show-progress \"https://raw.githubusercontent.com/aosokin/dl_cshse_ami/master/2021-fall/homeworks_small/shw5/utils.py\"\n","!wget --quiet --show-progress \"https://raw.githubusercontent.com/aosokin/dl_cshse_ami/master/2021-fall/homeworks_small/shw5/UNet.png\"\n","!wget --quiet --show-progress \"https://raw.githubusercontent.com/aosokin/dl_cshse_ami/master/2021-fall/homeworks_small/shw5/cityscapes_example.png\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iA7mLqbRuk-M"},"source":["# Metric Learning (0.5 баллов)"]},{"cell_type":"code","metadata":{"jupyter":{"outputs_hidden":true},"id":"BOllOSuduk-N"},"source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from IPython.display import clear_output\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision.datasets as dsets\n","import torchvision.transforms as transforms\n","import torch.optim as optim\n","from torch.utils.data.sampler import Sampler, BatchSampler\n","from torch.nn.modules.loss import MSELoss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y1wumt0buk-T"},"source":["Вам необходимо реализовать вычисление Contrastive Loss - одну из самых популярных функций потерь для metric learning. Contrastive Loss получает на вход пару векторов $x_i$ и $x_j$ (признаковые описания объектов $i$ и $j$, полученные нейросетью) и метку $y_{ij}$, причем $y_{ij} = 0$, если объекты \"похожи\" (принадлежат одному классу), и $y_{ij} = 1$, если объекты \"различны\" (принадлежат различным классам). Формально определим Contrastive Loss следующим образом:\n","\n","$$\n","L(x_i, x_j, y_{ij}) = (1 - y_{ij})\\|x_i - x_j\\|^2 + y_{ij}max(0, m - \\|x_i - x_j\\|^2)\n","$$\n","\n","где $m$ - гиперпараметр (его можно взять равным единице).\n","\n","Вместо того, чтобы формировать обучающее множество из всевозможных пар, можно поступить проще: будем пропускать батч из $N$ обучаюших изображений через сеть (тем самым получая соответствующие векторы $x$), а значение лосса вычислять как среднее значение функции $L$ на всех парах в этом батче. Тогда в обучении на каждом батче участвует $\\frac{N(N-1)}{2}$ пар, что существенно ускоряет сходимость на практике. Реализуйте предложенный вариант Contrastive Loss."]},{"cell_type":"code","metadata":{"jupyter":{"outputs_hidden":true},"id":"-r1_6Mq4uk-U"},"source":["class ContrastiveLoss(torch.nn.Module):\n","    def __init__(self, margin=1.0):\n","        super(ContrastiveLoss, self).__init__()\n","        self.margin = margin\n","\n","    def forward(self, x, y):\n","        # x: batch_size x num_features\n","        # y: batch_size\n","        #<your code>"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r4GLh9jsuk-Y"},"source":["В задачах metric learning, как правило, необходимо, чтобы количества \"положительных\" и \"отрицательных\" пар в обучении отличалось несильно. Поэтому в случае большого количества классов случайное формирование батчей неэффективно - в таком случае количество \"положительных\" пар очень мало. Поэтому будем формировать обучающие батчи размера $N$ следующим образом: будем брать $\\frac{N}{2}$ элементов из некоторого класса (они между собой будут формировать \"положительные пары\"), а оставшиеся $\\frac{N}{2}$ элементов будем брать случайно. Таким образом мы гарантируем, что в каждом обучающем батче будет достаточно \"положительных\" пар.\n","\n","Реализуйте предложенную логику в рамках Pytorch, реализовав собственный BatchSampler. Ваш самплер должен формировать каждый батч размера $N$ следующим образом: $\\frac{N}{2}$ объектов извлекаются из некоторого случайного класса, оставшиеся $\\frac{N}{2}$ объектов извлекаются случайно."]},{"cell_type":"code","metadata":{"jupyter":{"outputs_hidden":true},"id":"mD-soGxyuk-Z"},"source":["class ContrastiveSampler(BatchSampler):\n","    def __init__(self, batch_size, num_classes, labels):\n","        self.num_classes = num_classes\n","        self.imgs_per_class = labels.size()[0] // num_classes\n","        #<your code>\n","\n","    def __iter__(self):\n","        num_yielded = 0\n","        while num_yielded < (self.num_classes * self.imgs_per_class):\n","            batch = []\n","            #<your code>\n","            num_yielded += self.batch_size\n","            yield batch"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yiArWNrHuk-c"},"source":["В этом задании будем работать с небольшими изображениями одежды из датасета Fashion-MNIST."]},{"cell_type":"code","metadata":{"jupyter":{"outputs_hidden":true},"id":"pG9gtImnuk-d"},"source":["input_size = 784\n","num_classes = 10\n","batch_size = 256\n","\n","train_dataset = dsets.FashionMNIST(root='.', \n","                                   train=True, \n","                                   transform=transforms.ToTensor(),\n","                                   download=True)\n","\n","test_dataset = dsets.FashionMNIST(root='.', \n","                                  train=False, \n","                                  transform=transforms.ToTensor())\n","\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n","                                           batch_sampler=ContrastiveSampler(batch_size=batch_size, num_classes=num_classes, labels=train_dataset.train_labels), \n","                                           shuffle=False)\n","\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n","                                          batch_sampler=ContrastiveSampler(batch_size=batch_size, num_classes=num_classes, labels=test_dataset.test_labels), \n","                                          shuffle=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v08bnJfpuk-g"},"source":["Реализуйте сеть несложной архитектуры, содержащую три сверточных слоя из 20 фильтров с макс-пулингом, а также два полносвязных слоя из 128 нейронов. Выход последнего слоя будет подаваться на вход Contrastive Loss."]},{"cell_type":"code","metadata":{"jupyter":{"outputs_hidden":true},"id":"QC6vmcghuk-h"},"source":["class Flatten(nn.Module):\n","    def forward(self, x):\n","        return x.view(x.size()[0], -1)\n","\n","class ContrastiveNetwork(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.cnn1 = nn.Sequential(\n","            #<your code>\n","        )\n","\n","    def forward(self, x):\n","        output = self.cnn1(x)\n","        return output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"jupyter":{"outputs_hidden":true},"id":"bQyvGcKduk-k"},"source":["contrastive_loss = ContrastiveLoss()\n","\n","def train_epoch(model, optimizer):\n","    loss_log = []\n","    model.train()\n","    for batch_num, (data, target) in enumerate(train_loader):\n","        optimizer.zero_grad()\n","        output = model(data)        \n","        loss = contrastive_loss(output, target)\n","        loss.backward()\n","        optimizer.step()\n","        loss = loss.item()\n","        loss_log.append(loss)\n","    return loss_log   \n","\n","def test(model):\n","    loss_log = []\n","    model.eval()\n","    for batch_num, (data, target) in enumerate(test_loader):    \n","        output = model(data)\n","        loss = contrastive_loss(output, target)\n","        loss = loss.item()\n","        loss_log.append(loss)\n","    return loss_log\n","\n","def plot_history(train_history, val_history, title='loss'):\n","    plt.figure()\n","    plt.title('{}'.format(title))\n","    plt.plot(train_history, label='train', zorder=1)    \n","    points = np.array(val_history)\n","    plt.scatter(points[:, 0], points[:, 1], marker='+', s=180, c='orange', label='val', zorder=2)\n","    plt.xlabel('train steps')\n","    plt.legend(loc='best')\n","    plt.grid()\n","    plt.show()\n","    \n","def train(model, opt, n_epochs):\n","    train_log = []\n","    val_log = []\n","\n","    for epoch in range(n_epochs):\n","        print(\"Epoch {0} of {1}\".format(epoch, n_epochs))\n","        train_loss = train_epoch(model, opt)\n","        val_loss = test(model)\n","        train_log.extend(train_loss)\n","        steps = train_dataset.train_labels.shape[0] / batch_size\n","        val_log.append((steps * (epoch + 1), np.mean(val_loss)))\n","        clear_output()\n","        plot_history(train_log, val_log)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"jupyter":{"outputs_hidden":true},"id":"m1QpsRfpuk-q"},"source":["model = ContrastiveNetwork()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x_vBnt52uk-0"},"source":["Обучите сеть с параметрами, указанными ниже."]},{"cell_type":"code","metadata":{"jupyter":{"outputs_hidden":true},"id":"j0plnAAouk-1"},"source":["opt = torch.optim.Adam(model.parameters(), lr=0.0005)\n","train(model, opt, 2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-JW-AX-Nuk-4"},"source":["Извлеките векторные описания тестовых изображений (a.k.a эмбеддинги). У вас должно получиться 10000 128-мерных векторов."]},{"cell_type":"code","metadata":{"jupyter":{"outputs_hidden":true},"id":"Cg9jSvm4uk-5"},"source":["embeddings = #<your code>"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nofmBQ2luk-9"},"source":["Код ниже демонстрирует поисковую выдачу для трех изображений-запросов. Выдача формируется на основе близости эмбеддингов."]},{"cell_type":"code","metadata":{"jupyter":{"outputs_hidden":true},"id":"Zmq4bVNmuk-9"},"source":["queryCount = 3\n","queries = embeddings[:queryCount,:].data.numpy()\n","database = embeddings[queryCount:,:].data.numpy()\n","plt.figure(figsize=[15, 4.5])\n","for i in range(queryCount):\n","    results = np.argsort(np.sum((database-queries[i,:])**2, axis=1))[:10]\n","    plt.subplot(queryCount, 11, i * 11 + 1)\n","    plt.title(\"Query: %i\" % i)\n","    plt.imshow(test_dataset.test_data[i].numpy().reshape([28, 28]), cmap='gray')\n","    for k in range(10):\n","        plt.subplot(queryCount, 11, i * 11 + k + 2)\n","        plt.imshow(test_dataset.test_data[results[k]+queryCount].numpy().reshape([28, 28]), cmap='gray')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HPMg-415uk_B"},"source":["# Сегментация изображений (0.5 баллов)"]},{"cell_type":"markdown","metadata":{"id":"KVS9IYQiuk_B"},"source":["Сегментация изображения - это задача классификации каждого пикселя. Например, вот так (пример из датасета Cityscapes Dataset): \n","\n","<img src=\"cityscapes_example.png\" width=\"600\" height=\"500\">"]},{"cell_type":"markdown","metadata":{"id":"LyTXh52xuk_D"},"source":["В этом ноутбуке предлагается реализовать модель UNet для двухклассовой сегментации изображений."]},{"cell_type":"code","metadata":{"id":"9l6FELK4uk_D"},"source":["from torch.optim.lr_scheduler import StepLR\n","\n","from tqdm import trange"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EXm4re8buk_G"},"source":["from utils import WeizmannHorsesDataset, show_sample, plot_batch_with_results, plot_history, mean_accuracy, pixel_accuracy, mean_iou"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z5Y-e-Xquk_J"},"source":["### Скачивание и чтение данных"]},{"cell_type":"markdown","metadata":{"id":"aSF24Qc1uk_K"},"source":["В качестве датасета мы будем использовать датасет Weizmann Horse Database (источник: http://www.msri.org/m/people/members/eranb/ раздел - Horse Images), состоящий из черно-белых и цветных фотографий лошадей и разметки на два класса - foreground и background."]},{"cell_type":"code","metadata":{"id":"BQPzqyqGuk_K"},"source":["#! wget http://www.msri.org/people/members/eranb/weizmann_horse_db.tar.gz -O data.tar.gz\n","#! tar -xzf data.tar.gz"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EKYCTQftuk_M"},"source":["data_path = \"weizmann_horse_db\" \n","dataset = WeizmannHorsesDataset(data_path, \"test\", img_shape = (40,60), color = \"rgb\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e5aae-3Juk_O","outputId":"3ad6229e-cb66-4cc4-ddf0-51b065d8854e"},"source":["show_sample(dataset, 0)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAA88AAAEoCAYAAABmVcPQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3VmTZdl51vF3733mkyfnrMzKGrqreqge5VZLtmRZAhNY+IIxIAxccMEtV+YzcMMngE9AQIADAofNIISxrJalRrZ6UM/V1a2q6hpzHs589sBF24EjeJ+11OXqJEv9/12uFeuctddee1etPBHvk1RVZQAAAAAAQEv/f08AAAAAAIDTjsMzAAAAAAARHJ4BAAAAAIjg8AwAAAAAQASHZwAAAAAAIjg8AwAAAAAQweEZAAAAAICI2kl+2W//w2/JUOmmzdz28fFAft7e8dRtn+s05ZiN1TnZN5gUbvva0qIcc+7cquzb3jty239y7RM5ZlL6t+Ty5nk55sL6iuxrtetu+/lzG3LM9vaO2/76+x/KMdVoLPueuHTBbf+l55+QYxaW52VfWfrXtLPXl2Nu3brnttdr+u9HtZbeR9PJyG2fHftrZ2Z2/5P7bvvTV56WY37w5juyb9lfBsstkN1eZbLr+vau23400Ot6NPSf2+HIf5bMzHrdtuxbW+i47Vmqr6k/9edgZlZm/vWeF99jZnZn139ub20fyzHHo4nsG07999RqtyXHpJm/L0ezXI6Z5qXsm+X++pWVXtdmLZF933zBf6a/8eXLckx3zr/e8dh/lszM/um/+H09CeAz+nb6W4GXIwAA/9d3y99x/w/CL88AAAAAAERweAYAAAAAIILDMwAAAAAAERyeAQAAAACI4PAMAAAAAEDEiVbbPr+6JvuOJkO3/SfX7sgxG6JabXtJV9LdClSk/eZzfvXnLNV/Y2h0erLvqVW/Evf9sa6Ye3jgV62ea+mis2VdV4VeWvCri/fm9RrNSr/Sdaejt8v/fldXEJ+O/WrIL7+oK/MeHBzKvuGRX9n47ta+HHPzzpbb3u2IktVmtnFG79fZ1J/DUd/fx2Zmh0O/qvCdu34VbjOzyUTvlT98/5bbHiiSbFWi93JR+s/G4UBXQz6a+Pc2n+nn7GDoV582M/tk27+HxyM9ZkFUlDczm1X+PK7VdNXxo6F/b9NEjxnnurr42rxfXfyJc0tyzFLPH1OZvrkr4lk3M7t94FdMz6d63itd/V556vF1t73eaMgxZeHfi6qioDYAAHg08MszAAAAAAARHJ4BAAAAAIjg8AwAAAAAQASHZwAAAAAAIjg8AwAAAAAQcaLVtq9+clv2NRt+Jdtfe9GvgG1mttbxq21P67oq7p3dI9l3/uJZt3081BWU11aXZV+t7i/vs+f9KtxmZtXZym1fmteVdHsb5/QcKr+a7tLqohxjmT+HbqD67ihQbfjugb/mtUxX2R0NdHXlDz6+67aPJ36lcjNdSXo29a/VzOzaz/R+VdW7pxO/qrGZ2f6RX7X6tY/uyTHbfX1N9/b9iuRPrPnV0s3MntrUFZ4HovLyYk9XUFbXVJV6XY8mfjVrM7Nx7j+7nY7ee0ngeW+ZP4924PM2zvqVpNNCVx1fW9BV99dW/DW/tKGfwQvrC277QV/Pocr030LPiern3UCha1Ud28ys3fWrgddbeq80xHulUdf7AQAA4DThl2cAAAAAACI4PAMAAAAAEMHhGQAAAACACA7PAAAAAABEcHgGAAAAACCCwzMAAAAAABEnGlW11OvKvsurfhTTyqKOaBoM/YiT5SUd1XNuUc+hXvdzWwazXI6ZjXU00c6uHyuTByKVstSP3RmPdZxLR0QMmZk15upue62uY2jEFCyp9DosBu7tZOrHTt24fkuOef3d67Lvw9t7bvve8UCO6Q/9ezGc6EishbYfhWZmVooopqrS65qL6J8XntBRY/OBvfLyM+fd9svr+l6MB3qN5uf8Z201EH2lYsNqgT/L7R7rvby27sdEzSp/H5uZtXTqlM13/LW4eG5NjxHviFd/+Kocs7lxUX/euh9NtxZ4F9Uy/5mu7voRaWZmga1nyzV//RoWeHckOscqqYmXRCA2zHL/Gaybfq8AAACcJvzyDAAAAABABIdnAAAAAAAiODwDAAAAABDB4RkAAAAAgAgOzwAAAAAARJxote1uW39dS1S6VtWnzcw67Ybo0WVnh31dbXjn7o7bvnekK2onxUz2HU79efQ6uoJ4IpaoKnTl21qm12g49Ks137p2XY65d3Dktv/ojY/0mJ1j2ZeJMsD/6t//TzlmMNYVeNdXe277Uk+XXT6/5q/52qL/WWZmZ5Z03+vv33Tb+2K9zcwWOh23/WsvPinHbB/pauBPXNpw2w+3/QrYZmb1RV05e77nX+/C2pIcc3Dsz29pQa/d+kg/M49d9CuPH4rK+mZmjbp+r6yuLvtjan61dDOz2eDAbe+29f7avLgp+zYu+BXEF8/47WZmSd1/tzV71+WY4b5fhd7MrJ7613uwsy3HNBq6wvkk95/pblu/2zpN/z4d7euK3wAAAKcJvzwDAAAAABDB4RkAAAAAgAgOzwAAAAAARHB4BgAAAAAggsMzAAAAAAARHJ4BAAAAAIg40aiq4bGOidpO/OiTPBDR1Gz6cS55oWNt+kMd/XN004/4ubt9KMe8mevYnTT1l/eJCzrW5vaeH5Nz4/auHLO0/LHs2z7w576z78dRmZlNZn5M1M1tf25mZk+v6zijl5656LYvLXflmEpE4ZiZzc+33PaVnv681eUF/3tK/T37Rzp2qn888jtyHbG1tuLHJl258rgcs3EciHUS44YHfoSVmdnxlo6xGosIqTtb+r5nDT9+q9Vp6zGJ/ptdXvnr9/jTT8sxvUD8Vjn172Fa6nXNRQTexXP6/dVp6OirrZvX3fZWQ0Xtmc1tnnfbm01/75uZ5TW9rmnNj7PbPLsqx2SJfveWqT+PM1cek2PG236822Cg1xUAAOA04ZdnAAAAAAAiODwDAAAAABDB4RkAAAAAgAgOzwAAAAAARHB4BgAAAAAggsMzAAAAAAARJxpVtbOvI5+Swo8MGowKOWbrsO+27x4M5Zgy11FVZxf9+BUVjWRm1urqSJ6NRT/G5/0bt+WYP/34vtuemY6NSVtN2Vdv+rd4fk6PSa3utp9Z0evw5Qs68uYbL11y2/sTfS/e/Xhb9vX7fvzQ5mJPjkkzP6rncKKjqgYTvfdWF/x7m5U6sihN/Bim/s6OHLMTiMs6u7nmtncCe/IoEK3WbPv3N5vq+9QR+0tFRJmZdTt6H1155hm3ffXJy3LM+PhY9t372F/bNPBnw9nQn/u8iMYzMzve09FvmXjLzgIRTamIi1tY0nu8GOj366jvvxN76358mpmZVXr/F7n/PDUCUVqHlb8Quel3EQAAwGnCL88AAAAAAERweAYAAAAAIILDMwAAAAAAERyeAQAAAACI4PAMAAAAAEDEiVbbHo511d7X799y2+8d6srZg4lfkXappavifuPpM7Lva1fOuu1JoDpwqHLwcd+/3uee0hVzF9Y33PaGqBZtZnZ2dVH2DcYTt/1Hb30gx+Qjf82Xeroqbp7qqtVvXvOri6/O66rQe0cj2be2uuC21xp+lXAzs868v0aNlq4o3Ej15x0U/rjQX6PGE7/SdS3RFbprpX//zMxuXPvYbW+29CwagTLT05lfZfrwcE+Oqab+Oqxs6CrOc4Fnpl7411vs35Fj+lu7sq8Qe7nW1u+IiRhTiXtuZlaUev/Pz8+57dOhrtA9HfjXNApUFi9nen49MYfOot9uZpYUujJ7v+9/186te3LMWFSOLwNVvQEAAE4TfnkGAAAAACCCwzMAAAAAABEcngEAAAAAiODwDAAAAABABIdnAAAAAAAiODwDAAAAABBxolFVT172o6DMzO7c23Lbv/ysH91kZvbjd+667XN1Hes0C8XNmD+uW9fLVDcdUTPOE38Oid9upmOLJoHYmK2DQ9l3dDxw2z+87q+dmVk99dfheKTjfeqpXqO1pSW3fXN9RY4ZFzomaq7rRx315jpyTJX792k20ZFYw76OBep2/O9q1vUaTXP/3haFjqrqtPS6Fubvo6Oh3ivL83qN+gN/H9Vr+ppUVyg2TN0/M7PtO/6+VPFRZmZJTa9Rb8GPYioD9z0t/Ai8otTrmlT6mZ5N/ci6vX39eRMRO5Vl+l50xJ40M5tf8ePxaoF3ZVHp+WUiDi0f9OWYva37bvutOztyDAAAwGnCL88AAAAAAERweAYAAAAAIILDMwAAAAAAERyeAQAAAACI4PAMAAAAAEAEh2cAAAAAACJONKqqP9bRJxfO+TFWv/L8OTlm+3DittcDfxNYnNcxOb21Nbf90uM6UqnK9TXdH/jxW9OJjiZKxNzHMz/uxsys3e3KvuU1P6pnffWmHPPWe7fc9rv3dBzPXFNH6Cy+4EfoZImOyVlfmZd93Z5/vWfOrssxYz99yI6PdGTR6rK/H8zMGg3/ekcj8UVmNhv535VnOtZpXPp73MysmPgxZMOJnkMl4q3MzAYi4qpW189MPRORRZV+tcxEhJuZWaX2fx74O1+p4+Ly3I8bm4gINzOzw/7YbR+OdMzdXEdfby5uR1rT93009J/3dkuvXRKIlqqm/jXNEj2Hg6092Xf96jW3fSQitszMjob+HIpSvw8BfPF8584bn3nMb26+9DnMBAD+X/zyDAAAAABABIdnAAAAAAAiODwDAAAAABDB4RkAAAAAgAgOzwAAAAAARJxote0DUcXWzOxLVy657XNLC3JMq+FXil3o6OrT8y1d4bnT8CvZjg6O5JgkUOk3SfzPyws9piaK6c41dVXcdlevUWp+9dvljq6g/Kvri2771X1dofj9j+7Lvn8rqj8/87iujl2rN2Xf81cec9uL+lCOyXO/om+v51/rp326gni/71fOHhzq6t1l4v+tqtHW+3V87FeLNjMrRAXlyVRXXZ7mB7LveOx/XruhXxPNzH+ehoHK+sdDXQ282/Ersy9k+l5Mp7oS/eHBrts+HHz2atsrSz05JnS9W9t+9fp6oNp2q+t/14UNXfl/NtDvlf0j/x02f2ZVjjnYP5R9lXhPjUpdDVwVom/U9DsZwC+mB6mo/bA/jwrdAB4EvzwDAAAAABDB4RkAAAAAgAgOzwAAAAAARHB4BgAAAAAggsMzAAAAAAARHJ4BAAAAAIg40aiqlZ4fQ2Nm9uzTm257kflRS2ZmzYYfX5Nl+m8C9bq+5Lr5cUY3fqZjmDZWdXzN3FzbbS90CpPt7vf9jsKfm5nZ6pKOOtrd2nLbr133283M9g78OYynOmLo0oK+tyrG6g9+elOOqUxH3sy1/sRtP7++LMf85tdfdNt//SvPyjGHQx07NRSxTsVMr9FUREjVdMKQzQodw5SKbd6o6b0yGOvPK2b+RPb6Oi4rn0zc9rqanJllTR07tbLox67d29nXn5fo653N/PkdHOkYppl41npNfaPywJ8hR2KNup15OWa+669Rd1mPmY387/n0u/x3RKfjv6PMzDKVm2dm6xc23PZqV7y/zKwuIs+KQNwfgEfbw46kephOcm7EYgG/OPjlGQAAAACACA7PAAAAAABEcHgGAAAAACCCwzMAAAAAABEcngEAAAAAiDjRatuXNhdlX6/pV3a9fldX+p2W/tm/UeoqsVWgsOvWtl+Bt6r03xiSWl32PX7uMbd9FPi8d9+/7rb3j47kmOnuPdm39cZbbvtGoOr4gfqewLreOfarT5uZtUT180agKvog11XWD0RV4cMbd+WYoyO/CnA20vtreUVXNu7Oqz69RgeH/nf15nSl8lmpKyhXib9+7bb+vHKk99Fo6N/D3eOBHGPiPi12dEXtOVEl38xsPPC/q296DnPtluxLcn/9WoE/GybiHXF4qOfQ7gaq7vf8CuJrq6tyzMqy/65MRPVwM7NWW7+LFjb975rM/ArwZmYLK7p6fbPnvz92RKV+M7M08ffKYKLfHQBOv9NcUfu0UGtEFW7g0cMvzwAAAAAARHB4BgAAAAAggsMzAAAAAAARHJ4BAAAAAIjg8AwAAAAAQASHZwAAAAAAIk40qsqmuewqRn7EST7V8TCL3abbPsn13wQOpjqrKj3wv2upo6N/LNOxOyMRqXT/YCTHVGXltu/tqgApsw9ef1f2pQMxh0DszjD31ygUH2WB+K1NEU200PCv1cws1102E2tUmR50R1zvv/v+23LMP/r2y7JvbsGPqhoOdfRPlmZue7OmH8PJSMf45JX/PE1Fu5lZYf4czMxMRF/VxLzNzHqL/rOx1PafTTOzbiDGKp/597BV1zFMzYbus8z/vLLQ74EbO3tu+/VtHWvWrO/Lvq8+50fWXX68LcckDT/yrNHVsVyL6xuyr8ynbns+0O+iwwN9vYMtf19u3dXrkNX9/ZWlJ/vPEIDPjjiqz0doXYmxws+LfXSy+OUZAAAAAIAIDs8AAAAAAERweAYAAAAAIILDMwAAAAAAERyeAQAAAACI4PAMAAAAAEDEiWaE3N0fyr7B0I8+aZgf2WJmtrHsx+TcPgjEUYVifEQEUpHqOVhDR8f0D/zIm6Kv42Hub/ljfu/H78kxx4HPa6UiHqap44Iubqy47bWdQzlmJiKxzMwamb9+vbqOQBoGooTq4j7llR6j7vqtI70n/8P3fir7/u5f8Uv/v3DpghwznvjftX98JMe0WvoR3RP342jqx76ZmS3M6QipdtOPfCpLvVcaIgqqXtfznm/rZ2Zc8+PQysCrqqz0Pmq0e277H799U475/hsfu+39kR/3ZGZW5Dqi7Ifv/Mxtf+U1HTH3d771nNv+lZf9djOzpKGfz+27O377lo6Wund/W/aZeNaqQBRaUyTJdTt6TwLAnwtF7vwiRmkRP/SL6yT364N8F/srjF+eAQAAAACI4PAMAAAAAEAEh2cAAAAAACI4PAMAAAAAEMHhGQAAAACAiBOttj2r9Fl9Z2/gtm+LdjOzw75f4baR+VWDzczm6rqya1X5lX5rna4c8+FdXeG2f+zPvZbqqtBbR37l7JXFRTmm0dLzu3TOr5y9sjQvxxz0RbXmXJTLNbNsSfcVe/4abWS6Mm9e6c/rNvzqz2Wiq6JfPfCvqQhU9f5k+0D2/e4rb7rtzYZfAd7MbHXZX/Ob23oPNc3fk2Zm+2J/jaf6mjppYB+J6uflQD+DB2O/jnlzTT9n7+/pKs53d/01D1XvvrCur+nVD2+77a+8pattqz1RiSrvZma6x2zn2H+m//CtT+SY9277VbCv/PE1Oebc+rLsq6X+vd0/PJZjDgNV/C+s+u+cuW5g/y/5fb90ZVWOAfBoo2rv50NVUGa9406y+vSjWgX+NFToPs3V5vnlGQAAAACACA7PAAAAAABEcHgGAAAAACCCwzMAAAAAABEcngEAAAAAiODwDAAAAABARFIFIoEetn/+j/+a/LIXL/oxK7NAPFKa+mf/6cSPzzEzK03HGR2Pxm77XG9OjhmW+vMmUz9KK59M5Zjd4cRtX+625JhQ7NSt7V23fRiYw9Ub99z2XkPHD823dDzY8Y07bvtKIKpqGoidWm358/iw76+dmdn/uuXHI810qlNQSyQnPb2iY5POL/v7KC31fl3p+rFcZmbTwn82Pt4byjGzwt+TZmaLmf88PdbT9/3eyN9H6tk0M1vo6L18OPbndxi4UbOa3itv3Nxx2ycPcuNLPSYJxM+ptUgCezwT9yJL9Lpmgc9T7/kyENWmosvMzC6uLbjtX79yTo45t+mPWVrQ8Vb/7F/+Z31RwGf07fS3Tu4/PDg1HtW4IAAPz4PEW323/B33/yD88gwAAAAAQASHZwAAAAAAIjg8AwAAAAAQweEZAAAAAIAIDs8AAAAAAERweAYAAAAAIEIE7nw+jsY6Hun+sR9RI5JwzMyslvkpJlUg3KQrYo7MzNpdv304K+SYYaKjhCZiebPAqi90/CSNtUUxOTPb2FiSfTsH+277l558Uo45s+5Hynx89baew4KOaDqb+1FMF2Y6WqofiNApa/4CfjAYyTGZ2CsWiPdZbOq/Lb205seDXVnS92m16c97o9OWY9qBGKaf3Dty29/r63WoJ3pdX9jwrykUw/TxxH9ubx3pezvX1H01seRVIPpKxWWZma0u+nv5/v6hHFPk/vVmgWi1Rl0/1LXUv4dJIHZqVvlzyAv9LqoCL75Ww39PZXU9h81FvS+/9vSm2/74Of0uys2f+62tAzkGAADgL3qQ2KmHiV+eAQAAAACI4PAMAAAAAEAEh2cAAAAAACI4PAMAAAAAEMHhGQAAAACAiBOtth0o9GvTqV+J9TjX5/uGmH6noceEKuaWM39cUtMVtVfnOrLveORXFR6P9RxWOv4c9AzMbt33K2qbmbXFUmyu+lWIzcyOZgO3/W5Lb5dmJ1DivOdX7V3u6yrJ1VR/Xi4qGz8935NjDkd+VeiDiZ7Dt8+vyr5fubjstncyvffK0q+k3qrra+0EntCaqKC8M9IVmX92MJR9Nw79Kt1f3dAVxP/WU2fd9t+/tiXHXA/MoS2ez6W5lhyz2q7LvqGozH5mQe+Vwdh/bhvis8zCz9PGsv+OaDf0e2Ay8yvU39/zK6ybmU0n+gX71KXz/piB/ryltr7e9VW/Mvso0feiEO/rKvWfCwB4GFRl3u/ceeOEZ4IvmgepCv2w9+XDrEzNM/MpfnkGAAAAACCCwzMAAAAAABEcngEAAAAAiODwDAAAAABABIdnAAAAAAAiODwDAAAAABBxolFVVSDy6WDiR7NktaYc0xFRL0nhf5aZ2XDiRxaZmRXmRwY1azpKaL6pl7Df9yOf9g4O5Zjd0o9OOh+K1sl13NLxkf9dP3zjfTnGRNTXS889Jodcvbkj+/YnfvRPdeS3m5k9uajjkY7E/X1qXt+LZ+bX3PZbw7Ec88S8jiF79syc2z7NdezOJPejhBqhyKKpXqPJ1N9flxf1M7M31nvlk5Hf19v3I6zMzP7+ph/Z9aU1ff8OBvqamjV/75WljmFqpPpvgJ/s+/t/PvA8LYgotE5DxzCtBvbruXl/XJXq98ps6u+j+rK/78zMOmLeZmbLbf+7BoUe8+RlP97KzOybf+PX3PbmnF6HIvffvX/8yutyDAAAjyoV7RSKj3qY0VIP22me20nil2cAAAAAACI4PAMAAAAAEMHhGQAAAACACA7PAAAAAABEcHgGAAAAACDiRKttl4Eq2EnmVwhuNnRF2iz1K/D2Z/p7xn3d1xRVj2eVrqDc1IWzbTTyKzk3Ml1d2VJxSxJdbbgY6WrI6ga/89FtOabZ9u/F8aKuPr21cyz7+nt9t/3ljq6+Xg9UOO+LKt2Dqb63V5b8a1qs9JiNjn481sTUjzM976HoS0SFaTOzT7b9itpmZh/c89f16rHeD4dFIftUz0RvPRsfD932Tqqfmc1uS/blpT+Lvmg3M9sNTDBL/Wft4lm/+rqZWU/Mb6mn9/9SO1Tp2l+LXbF2Zmb1ll+hu9vQ1dIbgTSD0dRfv7nFRTlm5cyK7Jtv+d+l77pZKpIJnnv+UmAUAADA6cEvzwAAAAAARHB4BgAAAAAggsMzAAAAAAARHJ4BAAAAAIjg8AwAAAAAQASHZwAAAAAAIk40qqpW1xFNxwM/gqUodPhJ0u267YsLc3JM6PNWxLiD/V05Jp/ouJnZ2I8MKsczPWbqj9kaBCKQAhFNWeZf7ygw5vaOH4G0u3Ok5zDSETrJzI/JmV/qyTGDXM/vvli/x3o6qmdXRPV8eOTHiZmZfetJPy7IzGx+zv+uUWCNmiJtqQzEWx2M/FguM7MPDv298ua+jrfqB9Z1Q0SUPSFivszMuk1/7ssi9s3M7OKijqq6K+KbjvRjZqOJfp7Wlpfc9n/yt39djllZ9vdls6H3VyiGr6r8+c0C90JlPg3FO8XMLDW9X9Vbr9kK3NuW/tvq7dvbbvtopu9FKt7/hwO9xwEAAE4TfnkGAAAAACCCwzMAAAAAABEcngEAAAAAiODwDAAAAABABIdnAAAAAAAiODwDAAAAABBxolFVWVNHvdRTP/ImFe1mZnnpx6LsH+jokzTRn6cSg/oTkTFkZpbrvqEYlwSiiWpNP24mCaxDq6HjZrKqdNs3z7TlmPUzfqTMXFNvlyoPXFPdH7df6tidxb37sm+17c+vTPTfgq4N/L1ye6gjtoqZ3kf5xI+4ygPRUkcz/14cTXV82ut7OqPpzUM/kuoo159XVHqN7gz9uX/3Ex2/9dGRv37PL+u4uOOJjmhSQUcHgQikvNLXOyv8Z3A80fd9PPLv7WSi7+1Y7Aczsyzx7/ssEG9Vr/vvylBcVqOuI8BqDf8ZnARivorAPw/j0r+maSAKsBTvymZdv78A4PPym5svyb7v3HnjBGcC4FHCL88AAAAAAERweAYAAAAAIILDMwAAAAAAERyeAQAAAACI4PAMAAAAAEDEiVbbXlvqyb6aX0DZksD5flb4FV+zJFBtOFANdjwT1W8Dla5LURXXzGxpza9+26j5FbXNzFLzr6moApV5a7pabVNUug791SRNxLaodGXxek1vpXbbX4dk3Ndj3vMrSZuZvbXjVzb+eOdYjvnZ0B9zpq73w50jXUE5qfuVwm9s6+rYO6La9rsDXc36B3f1Ne1N/c+r1fX+urixIvsunV9z29/58KYc899uHbrtr+/qSuobrUDFaLGNStPPYBrYzWUuKpwf6TVPE1GBOlCpfzbR911MQVbANjNr1v3nfTAIPIOBqtVV6r9gU/XiNbPJvq5InsrK9nqNJrl/TeOprvgNAPhiUNXPqXyO04ZfngEAAAAAiODwDAAAAABABIdnAAAAAAAiODwDAAAAABDB4RkAAAAAgAgOzwAAAAAARJxoVNVcd053iiimSSDGpCr9WJREp69YEoibyXM/tqjT6coxs1xHxzRq/t8mskwve1n5uTatLBDv09ARNWXhz+/wWEcgWemvQ6utvycv9H2qiaivvVvbcsydW/uy74NDP0Jna6LncCDu7e5Ij/nXV/dk3+aa/3m1po6JGlf+frgbiE9bPn9W9j215D9P584syTGPnV2VffO9jtu+uaY/73e/86opGYpyAAAS5UlEQVTb3k5FPpOZ5YFn8LmnL7nt2Sd35Zgb23ovT8W+HAwncsz6mXm3PRfPppnZrNTPZ0e9BwIxUeo90Kj5sW9mZrl41s3MmuKdUxQ6Aq8ZmJ+K7coCL9+aiLOb6/j7DgBweqloqZP8nocdY0VcFn4e/PIMAAAAAEAEh2cAAAAAACI4PAMAAAAAEMHhGQAAAACACA7PAAAAAABEcHgGAAAAACDiRKOqskxH1AyGfqTMaOTHEpmZNZt+bEuzqS9rZ/9Q9tUzP2aoUdfxK+1AfJOJy011MpFlmf/3jCQQk6N7zNTfRxqtthxRzPx7kQYitpK6jup55/o9t/3VV9+RY7aP+7Lv4tl1t/2bz/sxR2Zm2/t+nNF//YGOH3jvWMdY/cpff9ZtXz+zKMeY+Te+DNzAJPTnrcr/vMlUxw9ldR2lNc39iTz52Joc81vf8NfhykBHjb12MJZ9Sdffl7/xV1+WY/Y//Fj2/cef3nLbv/enb8kxrZ7/Xd2W3uNJom/isC9isVJ9czsivqlW198zyvW7MlUvo8DeSwMvqqPh0G1fWtCxZvnMn1+aBF6IAPA5OamYo89jDicVE3XanVS01MOOywqN4d6efvzyDAAAAABABIdnAAAAAAAiODwDAAAAABDB4RkAAAAAgAgOzwAAAAAARJxote39A13pejLxK9KWukC3Nc2vHFxV+m8CSwtzsq+W+svRCFSSrmW6EncuKmRXpa4uWxSqInMhx4QqMs9EReZ6TQ+qpX4V87Gowm1mdueOX1HbzOyHP3nXbT8W99zM7NITj8m+rz3/hNu+tqzv7foZvwrwe9c/kWPKQMXvrfu7bvvSgr92ZmbTwl+/XNxzs/Bftxqi+vkk19W2x+OB7Gs1xfMUqMi8fG7Z/567eq8Md/Qcdq7ddNvrbf0Mbpw/I/t61+667T+96n+PmVm97q/6119+So5ZmtNV91PxvI/Gev+nouq+iXeemdksUGW9nPk3sRao+F0GNl9a8/fe4UA/M6n4rslUrwMA/GWdhqraeDSovcI9x1/EL88AAAAAAERweAYAAAAAIILDMwAAAAAAERyeAQAAAACI4PAMAAAAAEAEh2cAAAAAACJONKqq22rLvkxEPhWFjmiqiYymNPA3gWmuI3SKzI9zmQ6ncsxsqvvyQGSQ0mz661AG4q0mU71G05k/B/E1n0r8fLBprufw/Z+8J/smIibnW195UY5ZXerKvv7IjzqabI3kmEzE5Dx9cV2P2daL1D888juSi3JMr+PHGSWm13WW65yoRt1/fJuBfVcGvqte8683DWShTWd+hNTxhs6Ye+etW7JvV6xrMr8lx5z55Suy70svPeO23/3R23LMT9694bYv9DpyzEvPbMq+RsNfo1Y7EG8l1nw40u+bKvCutLofcTUW7wczs5qMyzJLMhUdpveXJX5flun4LQD4eTxIHNVpiB8KzSF0TUQqhT3ouj7ImJP6Lu7t6cEvzwAAAAAARHB4BgAAAAAggsMzAAAAAAARHJ4BAAAAAIjg8AwAAAAAQMSJVtueFrqya1X6VYVTUfn508/zq8tOCl3xtax09WITlY1DVWcns4nsqyq/enG7qZe9Iarihioedzp6jcrEn0Nmeh3Kwu8bTfQYVVHbzOyXn3/KbT93dlmOURWKzcwSVWU9DVQbrvw98USjJYf8eFdU1DazwdaB235loKsh12qqqrC+t5W+tZaXfmcqKtebmY1Ger+OxqoSvV5XVaH7cKzHDALV4ZdXFt32Z5/WVczTwPN5+dI5t/03cj2H3/vBT932m7e39fec03s5zfw1T1J9c5uikvpBfyjHZKm+7ws9P+mg09ZV7UPvvZqonF0Fqm1PZv6ai0cTAB4KqhTjL+tB99CDjFPVth+04jcePn55BgAAAAAggsMzAAAAAAARHJ4BAAAAAIjg8AwAAAAAQASHZwAAAAAAIjg8AwAAAAAQcaJRVZUFcndS/xzfbukooVrNn36a6suqKh1Rk+d+lFYWiP7pdPwIGDOzxPxoolqm10F9VyjOpQrEb1UqqioQk5OKuJmavhV27uyK7LuwecZt73T12jXqes1VJM80H8kx6mrnA5FYKxtrsu+dP3nPbf/ZnS055uxswW2fFSoiyiwJ3PhmS0Vf6Xs7Ho1lXyrWda6j10jt/6Wz5+WYtZWPZN+Xnn3cbW819X4YBq5Jxa5tntP39utfvuK2v331uhxTBDLFuq2mP7dSv4umU/9dFHrfzHU6si8T79dAqp/tBWKxWg1/75UiPtDMrBJrNAzEpwHAF1UofigUW4QHX5/TEPmk5sA9Pz345RkAAAAAgAgOzwAAAAAARHB4BgAAAAAggsMzAAAAAAARHJ4BAAAAAIjg8AwAAAAAQMSJRlVd/eiG7Ot2u357W8fkLM333PZm04+GMTOri4gVM7N6XfQF4oKqQNxMWU3d9pGIoTEzSxI/ziUVUTNmZmmmb2OS+N81C0TKmIiqeuLZL8kRv/2SLu//2it/5LZPpzpiaDbV8zs8Grjtg4mOvOmIuKDlRX8PmZlduazjlu5sH7rtWzvHcsxj59fd9m5NRwzVQpFiok9sITMzW+rNyT41rNnUz+DXfv033PZWT6/r3tYd2ZeKZ2000XtF7Vczs6Z4ppNARNNLzzzmtn/1a1+VY5bn9HvlaNu/3mmub9RMpM8lgf0wCER2tUQkWxKKDyx1BN5o7MerjUY6Lm5xYd5tD0XWAQB+fqE4o9MQw3QasA74y+KXZwAAAAAAIjg8AwAAAAAQweEZAAAAAIAIDs8AAAAAAERweAYAAAAAIOJEq22vrfgVtc3MGjW/GnKnqysRZ5lfKXY01hVfZ4VfJdbMTHxcsMLtbKarQqvK2ZOJX4XbTFfVDlUQt0RXxU3FHPJcr0MhKojPzek57O/flX2Hh7tueydwTVXg7zqDsV9VW90/M7N66pdXHg77ckwt1RWUX37mgtv+2rvX5ZgPRbX5x86tyDGNWib7ZrlfSb0TqI7dDlSvH039fXn24uNyzIXLfmXq//67/0mOsVJXm69E9Wdx+8zMLAnc+EpU4q5MPzN1MYdf/cpzcsyFJ5+Sff/ld/6N254P9HuqW/P3XhXY40mqX+f1zL/voffAdKL7BkP/GcxzfaOO7u277ap6PgD8vL5oFZTV9Yaqbf8iUtf7RdsPVFk/WfzyDAAAAABABIdnAAAAAAAiODwDAAAAABDB4RkAAAAAgAgOzwAAAAAARHB4BgAAAAAg4kSjqkZjHX0yLPx4pDzXUVCZSPEZTXQUTjMQ49Nt+9FJtUT/jUHFBZmZ5YUfhxMak2X+dw0GgUilQJzRaOJHyjRFFI6ZWSb6emJ9zMyuvq+jqqrKj685FpFTZmZFrqOEaiKaqBaIFEsTfw71ul67MhCP1O34a/GVX3pSjvnha++67Y2mnsNTj5+Vfe3En4OKcDMz00+TWaftx8I998KzcszrP/ojt337jt4Pi0uLehKlf99Df+ULxTfNROxaKd43ZmaV6Hv7jTflmMvP6DXauHjRbf/e916VY7Z3D9324Dt0pPvywl+k8TQQVTXV7ym9fPqhEal5Ngs86wDw54jjiQutw6Ma68R9/9SD3Ft8PvjlGQAAAACACA7PAAAAAABEcHgGAAAAACCCwzMAAAAAABEcngEAAAAAiODwDAAAAABAxIlGVSVp4KwuckzyQKRMveHHTjWa+num06n+PBHxMwhEqRSBKK16zZ9fISKszMwqEfVSq+lbVYooKDOzdlPEGQXit2qp/3lvv/I/5JiP7uzJvvn5OX8OKmvMwpFKifn3qRIxR2ZmRenHYuXTsRwzmupZqFvYbrXlmAvry277jbu7ckyroe/76kLXbR+M9B7fPtKRZ72Gfz9evPqaHPPdV/x4hKyuY80Kcf/MzKZT/z7V64H9X+j9r+LQJhO9RlPRN5t+Isds7er9//f+wd90299962055qfvXXfbA2lsVgvEz81m/l4uZvqZaTX0l7Vb/ne1WjoKsCbu4XCg7wUA4OFQUUePchTUoxq/9bA97BirL9r6fVb88gwAAAAAQASHZwAAAAAAIjg8AwAAAAAQweEZAAAAAIAIDs8AAAAAAEScaLXtWqq/rjK/6msVON6PRaXrotRVYlvtjuwrK38O9ZquCl3L9ARVEexaXVekLSpxTWJuZmZpoGp1Kap3h8bMRPvOWM+h09bVlXcO/ArPSaXvU5Ho70rE33zqtUAF8brf1xYV2z8do9fISn9dQ/fp8sWzbvvCnF8128xse+9A9hVyDrr6dDNwveoZfOOda3LMUFQkz0e6qncaKBmdZn4V58B2Dd73Xtu/3m5Lv4saqV8dvhZ4D0zH+np//P0/cNvr06Ec88Jlf69McvV0mk1mgVLc4j3QDlTHbtZ19W51C7PAGjVa/juiFqgoDwB/jgrAJ++0V+I+DXM47R6kyjrC+OUZAAAAAIAIDs8AAAAAAERweAYAAAAAIILDMwAAAAAAERyeAQAAAACI4PAMAAAAAEDEiWaEdBcXZV+aqIgTPwrHzKwSsU5lEYjCSfXfC0oR/RNIVLKyyHVn6ke9pIF4q0RENBXTqZ6DiBgyM6tEbFE+mejPE0lHWSBiKBTj02n6cThbe8dyTKOpPy8vRJxXocfUS3/N1bWamVWVvreJyFCrAmtUzvx72JsLxAU1l2VfLfWvNwvsr0Cqk4x8Ggf+xrZ5Zt5tr0yvQ5J89r/ZJYF1rQLRdOr2Bl4DclSS6M0yGY9k363b9/3PC6xrW0VpTfUc0kyvw1yn7bY3GzqOqhF4pivx4ARuhTVFnF2joWPuAACfr9Me93Ta5/eoYl0fHL88AwAAAAAQweEZAAAAAIAIDs8AAAAAAERweAYAAAAAIILDMwAAAAAAESdabXsy05WzVYVgUSz6zzr90q5poJpvZbqCrImKvqpitZmZBSrSmqjIXJS6OraqJD2bzj7zGDNdpbie6Vtfq/l9o1JfaxJa16Zf0XftTEsOSRNdtreo/PULVcc2USk5ERWrP6UrEaeiIrOqQvzpGH/ejbqeQ7up71Mq7mGomnU9UJFZ7vNAaeo089coCdy/UPn6SqxrWQaq7geuSc29DFUDV/dW7DszsyoNvEpTcU2BdV1a67rti4F5l+J9Y6bfo+pag4PMrF4TSQKhquiiPXRvAQAAThN+eQYAAAAAIILDMwAAAAAAERyeAQAAAACI4PAMAAAAAEAEh2cAAAAAACI4PAMAAAAAEHGiUVWzXEe9TPKJ2x4IobFERKnkMx1ZVKsH4odEdEwoLSsUzaJmH4phqkTMUFJr6DlkeoYNESmj4qjMzAqxrrPAQoRWQQ1LAilRag6ffpc/MMmaeoycoN6ToWtS9z20H5JERGwFost0j1klIp9C8y6KwLqqiKvQJMTdTUKzCNxbed9D8XOhfSnvUyB+K/UvOBQBljbbsq/W9CPZ1LNuZlaKiyoDUWgWiF1TaVDBt1fgT6vDqf+OTQJxXuq9F9wrAAAApwi/PAMAAAAAEMHhGQAAAACACA7PAAAAAABEcHgGAAAAACCCwzMAAAAAABEcngEAAAAAiDjRqKrxbPaZx6SBGBMVfVIF4oImeSDGKvOXQ8XGmFkw66USsS2hmBwVrRMUyJQRCTVmRSgeyb9edT1mZqEEHRORSqEhwVVQaUaBSSSJilQK3dtA7FTx2e+T+rhQBFIV2HtqGwWvKUBHNIXit8S6Bpfns0e1hf7OVwWuNylFVFVgiTIZEyWfJqvX9DXlYl9Op348n5nJBUzFO8rMrAytq7zezx4t9WmnP48q8AxWYsMSVAUAAB4V/PIMAAAAAEAEh2cAAAAAACI4PAMAAAAAEMHhGQAAAACACA7PAAAAAABEnGi17Vmo2raqshuq9CvqtIaqA4dquxaVqKYbqDId+i5VdzYpAxVuRbXtKlQVN1C9W1VrLgJjlCRQ+Xk4GMq+elb3O2pZ4MsepJp1qCq0v36hatbBzzN/7qFZV7IydXCUnoOoJK2eiz/r1ESl5NDnqcrelXqWzMwCFeXVWmSp3ivhreLv81Al6aIUFfkD74FZqMp0qZ41fU2VqOydBt4DldjjZoFq+KEkAf1xVlMV9APvtlwlHVBuGwAAPCL45RkAAAAAgAgOzwAAAAAARHB4BgAAAAAggsMzAAAAAAARHJ4BAAAAAIjg8AwAAAAAQEQSiuoBAAAAAAD88gwAAAAAQBSHZwAAAAAAIjg8AwAAAAAQweEZAAAAAIAIDs8AAAAAAERweAYAAAAAIILDMwAAAAAAERyeAQAAAACI4PAMAAAAAEAEh2cAAAAAACI4PAMAAAAAEMHhGQAAAACACA7PAAAAAABEcHgGAAAAACCCwzMAAAAAABEcngEAAAAAiODwDAAAAABABIdnAAAAAAAiODwDAAAAABDB4RkAAAAAgAgOzwAAAAAARHB4BgAAAAAggsMzAAAAAAAR/wdDi9JzChD3/AAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 1080x288 with 2 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"p5M19xupuk_R"},"source":["## Модель"]},{"cell_type":"markdown","metadata":{"id":"L4t2N10_uk_S"},"source":["Схема модели UNet из оригинальной статьи (https://arxiv.org/abs/1505.04597 ):"]},{"cell_type":"markdown","metadata":{"id":"lgPV8VvJuk_S"},"source":["<img src=\"UNet.png\" width=\"600\" height=\"500\">"]},{"cell_type":"markdown","metadata":{"id":"xxcDlcf6uk_T"},"source":["На изображении выше синие блоки - карты признаков (feature maps). \n","\n","На вход модель принимает исходное изображение, результат модели - сегментация входного изображения (классификация каждого пикселя).\n","\n","Модель состоит из энкодера (на схеме слева) и декодера (справа).\n","\n","#### О реализации:\n","\n","Несколько одинаковых слоев сети, обрабатывающих feature map без изменения размера (например, повторение слоев Conv2d, BatchNorm2d, ReLU) реализованы в классе **ConvBlock**. На изображении один ConvBlock соответствует нескольким подряд идущим синим стрелкам.\n","\n","В реализации можно не делать crop при копировании feature map, а использовать padding, чтобы размеры карт признаков на одних и тех же уровнях декодера и энкодера совпадали.\n","\n","Для увеличения размера карты признаков при декодировании (**UpBlock**) вам могут понадобиться nn.ConvTranspose2d или nn.Upsample.\n","\n","В каждом следующем кодирующем/декодирующем слое **DownBlock**/**UpBlock** предполагается уменьшение/увеличение x-y размеров в $c_1$ раз и увеличение/уменьшение количества каналов в $c_2$ раз. Например, можно взять $c_1 = c_2 = 2$."]},{"cell_type":"code","metadata":{"id":"s7qdkRJSuk_U"},"source":["class ConvBlock(nn.Module):\n","    '''\n","       Convolutional Block, includes several sequential convolutional and activation layers.\n","       Hint: include BatchNorm here\n","    '''\n","    def __init__(self, input_ch, output_ch, kernel_size, block_depth):\n","        '''\n","            input_ch - input channels num\n","            output_ch - output channels num\n","            kernel_size - kernel size for convolution layers\n","            block_depth - number of convolution + activation repetitions\n","        '''\n","        super().__init__()\n","        \n","        # your code\n","        # conv_list = \n","\n","        self.conv_net = nn.Sequential(*conv_list)\n","\n","    def forward(self, x):\n","        x = self.conv_net(x)\n","        return x\n","    \n","    \n","class DownBlock(nn.Module):\n","    '''\n","        Encoding block, includes pooling (for shape reduction) and Convolutional Block (ConvBlock)\n","    '''\n","    def __init__(self, input_ch, output_ch, kernel_size, block_depth):\n","        super().__init__()\n","        \n","        # your code\n","        # self.layers = \n","\n","    def forward(self, x):\n","        x = self.layers(x)\n","        return x\n","\n","\n","class UpBlock(nn.Module):\n","    '''\n","        Decoding block, includes upsampling and Convolutional Block (ConvBlock)\n","    '''\n","    def __init__(self, input_ch, output_ch, kernel_size, block_depth):\n","        super().__init__()\n","        \n","        # your code\n","\n","    def forward(self, copied_input, lower_input):\n","        '''\n","            copied_input - feature map from one of the encoder layers\n","            lower_input - feature map from previous decoder layer\n","        '''\n","        # your code\n","        \n","        return x\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ESt62aWeuk_X"},"source":["class UNet(nn.Module):\n","    def __init__(self, n_classes, feature_levels_num, input_ch_size, hidden_ch_size, block_depth, kernel_size = 3):\n","        \"\"\"\n","        Input:\n","            n_classes - number of classes\n","            feature_levels_num - number of down- and up- block levels\n","            input_ch_size - input number of channels (1 for gray images, 3 for rgb)\n","            hidden_ch_size - output number of channels of the first Convolutional Block (in the original paper - 32)\n","            block_depth - number of convolutions + activations in one Convolutional Block\n","            kernel_size - kernel size for all convolution layers\n","        \"\"\"\n","        super(UNet, self).__init__()\n","        self.input_block = ConvBlock(input_ch_size, hidden_ch_size, 1, block_depth)\n","        self.down_blocks = []\n","        self.up_blocks = []\n","        self.feature_levels_num = feature_levels_num\n","        \n","        \n","        cur_ch_num = hidden_ch_size\n","        for _ in range(feature_levels_num):\n","            # your code\n","            # fill self.down_blocks and self.up_blocks with DownBlock/UpBlock\n","            # each DownBlock/UpBlock increase/decrease number of channels by 2 times\n","        \n","        self.down_blocks = nn.ModuleList(self.down_blocks)\n","        self.up_blocks = nn.ModuleList(self.up_blocks)\n","        self.output_block = ConvBlock(hidden_ch_size, n_classes, 1, block_depth)\n","\n","    def forward(self, x):\n","        x = self.input_block(x)\n","        \n","        # your code\n","        \n","        x = self.output_block(x)\n","        return x  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RlnS-6Yiuk_a"},"source":["## Метрики качества"]},{"cell_type":"markdown","metadata":{"id":"Kts-Pv3luk_b"},"source":["Для сегментации изображения размера $H \\times W$ на множество классов $С$ могут быть рассмотрены следующие метрики:"]},{"cell_type":"markdown","metadata":{"id":"W-F7GLffuk_b"},"source":["$$Mean~IoU = \\frac{1}{|C|}\\sum_{i \\in C}\\frac{TP_i}{FP_i + TP_i + FN_i}$$\n","\n","$$Pixel~Accuracy = \\frac{\\sum_{i \\in C}TP_i}{H\\times W}$$\n","\n","$$Mean~Accuracy = \\sum_{i \\in C}\\frac{TP_i}{TP_i + FN_i}$$"]},{"cell_type":"markdown","metadata":{"id":"AVBdF1t_uk_c"},"source":["## Обучение"]},{"cell_type":"code","metadata":{"id":"MX2tKjrRuk_c"},"source":["from collections import defaultdict"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HE6bczIDuk_f"},"source":["def train_epoch(model, optimizer, train_loader):\n","    loss_log = []\n","    model.train()\n","    for _, (x_batch, y_batch) in zip(trange(len(train_loader)), train_loader):\n","        optimizer.zero_grad()\n","        prediction = model(x_batch)\n","        loss = nn.CrossEntropyLoss()(prediction, y_batch.squeeze())\n","        loss.backward()\n","        optimizer.step()\n","        loss_log.append(loss.item())\n","    return loss_log\n","\n","def test(model, test_loader, train_loader):\n","    '''\n","        Computes metrics on both train and test, loss on test\n","    '''\n","    history = defaultdict(list)\n","    model.eval()\n","    \n","    for batch_num, (x_batch, y_batch) in zip(trange(len(test_loader)), test_loader):\n","        pred = model(x_batch)\n","        ce_loss = nn.CrossEntropyLoss()(pred, y_batch.squeeze()).item()\n","        pred = torch.argmax(pred, dim = 1) #before: (bs, classes, h, w)\n","        iou_sum = 0\n","        pixel_acc_sum = 0\n","        mean_acc_sum = 0\n","        for i in range(x_batch.shape[0]):\n","            iou_sum += mean_iou(pred[i].cpu().squeeze(), y_batch[i].cpu().squeeze())\n","            pixel_acc_sum += pixel_accuracy(pred[i].cpu().squeeze(), y_batch[i].cpu().squeeze())\n","            mean_acc_sum += mean_accuracy(pred[i].cpu().squeeze(), y_batch[i].cpu().squeeze())\n","            \n","        history[\"loss_val\"].append(ce_loss)\n","        history[\"iou_val\"].append(iou_sum/x_batch.shape[0])\n","        history[\"pixel_acc_val\"].append(pixel_acc_sum/x_batch.shape[0])\n","        history[\"mean_acc_val\"].append(mean_acc_sum/x_batch.shape[0])\n","        \n","        if batch_num == 0:\n","            plot_batch_with_results(x_batch.cpu(), y_batch.cpu(), pred.cpu()) \n","            \n","    for batch_num, (x_batch, y_batch) in zip(trange(len(train_loader)), train_loader):  \n","        pred = torch.argmax(model(x_batch), dim = 1)\n","        iou_sum = 0\n","        pixel_acc_sum = 0\n","        mean_acc_sum = 0\n","        for i in range(x_batch.shape[0]):\n","            pred_squeezed = pred[i].squeeze()\n","            y_squeezed = y_batch[i].squeeze()\n","            iou_sum += mean_iou(pred_squeezed, y_squeezed)\n","            pixel_acc_sum += pixel_accuracy(pred_squeezed, y_squeezed)\n","            mean_acc_sum += mean_accuracy(pred_squeezed, y_squeezed)\n","        \n","        history[\"iou_train\"].append(iou_sum/x_batch.shape[0])\n","        history[\"pixel_acc_train\"].append(pixel_acc_sum/x_batch.shape[0])\n","        history[\"mean_acc_train\"].append(mean_acc_sum/x_batch.shape[0])\n","\n","    return history\n","    \n","def train_procedure(model, opt, n_epochs, train_loader, test_loader, scheduler):\n","    history = defaultdict(list)\n","    steps = len(train_loader)\n","\n","    for epoch in range(n_epochs):\n","        \n","        train_loss = train_epoch(model, opt, train_loader)\n","        test_history = test(model, test_loader, train_loader)\n","        \n","        history[\"loss_train\"].extend(train_loss)\n","        history[\"loss_val\"].append((steps * (epoch + 1), np.mean(test_history[\"loss_val\"])))\n","        \n","        for key in test_history:\n","            if (key != \"loss_val\"):\n","                history[key].append(np.mean(test_history[key]))\n","                \n","        plot_history(history)\n","        \n","        print(\"Epoch average loss:\", np.mean(train_loss))\n","        print(\"Epoch validation metrics:\")\n","        print(\"\\tIoU -\", history['iou_val'][-1])\n","        print(\"\\tmean accuracy -\", history['mean_acc_val'][-1])\n","        print(\"\\tpixel acc -\", history[\"pixel_acc_val\"][-1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XPD_u5GYuk_h"},"source":["Подберите параметры сети, обучите модель. Если все сделано правильно, то итоговое значение IoU должно превосходить 0.8."]},{"cell_type":"code","metadata":{"id":"0ZmOq7ifuk_i"},"source":["#your parameters\n","n_classes = 2\n","shape = (40,60)\n","input_ch_size = 3\n","hidden_ch_size = \n","feature_levels_num = \n","block_depth = \n","\n","batch_size = 8\n","n_epochs = 25\n","lr = 0.05\n","lr_step = 10 \n","lr_gamma = 0.5"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bKviKokouk_q"},"source":["train_dataset = WeizmannHorsesDataset(root=data_path, split=\"train\", img_shape = shape)\n","test_dataset = WeizmannHorsesDataset(root=data_path, split=\"val\", img_shape = shape)\n","\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle = True)\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle = True)\n","\n","model = UNet(n_classes, feature_levels_num, input_ch_size, hidden_ch_size, block_depth)\n","\n","opt = torch.optim.Adam(model.parameters(), lr=lr)\n","scheduler = StepLR(opt, step_size=lr_step, gamma=lr_gamma)\n","train_procedure(model, opt, n_epochs, train_loader, test_loader, scheduler = scheduler)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"jupyter":{"outputs_hidden":true},"id":"m60O2Iahuk_t"},"source":[""],"execution_count":null,"outputs":[]}]}
